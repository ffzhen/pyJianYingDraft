#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ç«å±±å¼•æ“è¯­éŸ³è¯†åˆ«é›†æˆ
ä½¿ç”¨ç«å±±å¼•æ“ASRæ¥å£æ›¿ä»£Whisperè¿›è¡ŒéŸ³é¢‘è½¬å½•
"""

import time
import requests
import json
from typing import Dict, Any, List, Optional


class VolcengineASR:
    """ç«å±±å¼•æ“è¯­éŸ³è¯†åˆ«å®¢æˆ·ç«¯"""
    
    def __init__(self, appid: str, access_token: str, doubao_token: str = None, doubao_model: str = "doubao-1-5-pro-32k-250115"):
        """åˆå§‹åŒ–ç«å±±å¼•æ“ASRå®¢æˆ·ç«¯
        
        Args:
            appid: ç«å±±å¼•æ“ASRåº”ç”¨ID
            access_token: ç«å±±å¼•æ“ASRè®¿é—®ä»¤ç‰Œ
            doubao_token: è±†åŒ…APIè®¿é—®ä»¤ç‰Œï¼ˆç”¨äºå…³é”®è¯æå–ï¼‰
            doubao_model: è±†åŒ…æ¨¡å‹åç§°ï¼Œé»˜è®¤ä¸ºdoubao-1-5-pro-32k-250115
        """
        # ç«å±±å¼•æ“ASRé…ç½®
        self.base_url = 'https://openspeech.bytedance.com/api/v1/vc'
        self.appid = appid
        self.access_token = access_token
        
        # è±†åŒ…APIé…ç½®ï¼ˆç”¨äºå…³é”®è¯æå–ï¼‰
        self.doubao_token = doubao_token
        self.doubao_model = doubao_model
        
    def submit_audio_file(self, file_url: str, language: str = 'zh-CN') -> Optional[str]:
        """æäº¤éŸ³é¢‘æ–‡ä»¶è¿›è¡Œè¯†åˆ«
        
        Args:
            file_url: éŸ³é¢‘æ–‡ä»¶URL
            language: è¯­è¨€ä»£ç ï¼Œé»˜è®¤ä¸­æ–‡
            
        Returns:
            ä»»åŠ¡IDï¼Œå¤±è´¥è¿”å›None
        """
        print(f"[INFO] æäº¤éŸ³é¢‘æ–‡ä»¶è¿›è¡Œè¯†åˆ«: {file_url}")
        
        try:
            response = requests.post(
                f'{self.base_url}/submit',
                params={
                    'appid': self.appid,
                    'language': language,
                    'use_itn': 'True',           # å¯ç”¨é€†æ–‡æœ¬æ ‡å‡†åŒ–
                    'use_capitalize': 'True',    # å¯ç”¨é¦–å­—æ¯å¤§å†™
                    'max_lines': 1,              # æ¯è¡Œæœ€å¤š1å¥
                    'words_per_line': 10,        # æ¯è¡Œæœ€å¤š15è¯
                },
                json={
                    'url': file_url,
                },
                headers={
                    'content-type': 'application/json',
                    'Authorization': f'Bearer; {self.access_token}'
                }
            )
            
            if response.status_code == 200:
                result = response.json()
                if result.get('message') == 'Success':
                    job_id = result.get('id')
                    print(f"[OK] ä»»åŠ¡æäº¤æˆåŠŸï¼Œä»»åŠ¡ID: {job_id}")
                    return job_id
                else:
                    print(f"[ERROR] ä»»åŠ¡æäº¤å¤±è´¥: {result}")
                    return None
            else:
                print(f"[ERROR] HTTPé”™è¯¯: {response.status_code}, {response.text}")
                return None
                
        except Exception as e:
            print(f"[ERROR] æäº¤éŸ³é¢‘æ–‡ä»¶å¼‚å¸¸: {e}")
            return None
    
    def query_result(self, job_id: str) -> Optional[Dict[str, Any]]:
        """æŸ¥è¯¢è¯†åˆ«ç»“æœ
        
        Args:
            job_id: ä»»åŠ¡ID
            
        Returns:
            è¯†åˆ«ç»“æœï¼Œå¤±è´¥è¿”å›None
        """
        try:
            response = requests.get(
                f'{self.base_url}/query',
                params={
                    'appid': self.appid,
                    'id': job_id,
                },
                headers={
                    'Authorization': f'Bearer; {self.access_token}'
                }
            )
            
            if response.status_code == 200:
                result = response.json()
                print(f"[INFO] æŸ¥è¯¢å“åº”: {json.dumps(result, ensure_ascii=False, indent=2)}")
                return result
            else:
                print(f"[ERROR] æŸ¥è¯¢HTTPé”™è¯¯: {response.status_code}, {response.text}")
                return None
                
        except Exception as e:
            print(f"[ERROR] æŸ¥è¯¢ç»“æœå¼‚å¸¸: {e}")
            return None
    
    def wait_for_completion(self, job_id: str, max_wait_time: int = 300) -> Optional[Dict[str, Any]]:
        """ç­‰å¾…è¯†åˆ«å®Œæˆ
        
        Args:
            job_id: ä»»åŠ¡ID
            max_wait_time: æœ€å¤§ç­‰å¾…æ—¶é—´ï¼ˆç§’ï¼‰
            
        Returns:
            æœ€ç»ˆè¯†åˆ«ç»“æœï¼Œå¤±è´¥è¿”å›None
        """
        print(f"[INFO] ç­‰å¾…è¯†åˆ«å®Œæˆï¼Œæœ€å¤§ç­‰å¾…æ—¶é—´: {max_wait_time}ç§’")
        
        start_time = time.time()
        wait_interval = 5  # æ¯5ç§’æŸ¥è¯¢ä¸€æ¬¡
        
        while time.time() - start_time < max_wait_time:
            result = self.query_result(job_id)
            
            if result is None:
                print("[ERROR] æŸ¥è¯¢å¤±è´¥")
                return None
            
            # æ£€æŸ¥æ˜¯å¦æœ‰utterancesæ•°æ®ï¼Œå¦‚æœæœ‰å°±è¡¨ç¤ºæˆåŠŸ
            utterances = result.get('utterances', [])
            code = result.get('code', -1)
            message = result.get('message', '')
            
            print(f"[INFO] å½“å‰çŠ¶æ€ç : {code}, æ¶ˆæ¯: {message}")
            
            if code == 0 and utterances:
                print("[OK] è¯†åˆ«å®Œæˆ!")
                return result
            elif code != 0:
                print(f"[ERROR] è¯†åˆ«å¤±è´¥! é”™è¯¯ç : {code}, æ¶ˆæ¯: {message}")
                return None
            else:
                print(f"[INFO] è¯†åˆ«è¿›è¡Œä¸­ï¼Œç­‰å¾…{wait_interval}ç§’åé‡è¯•...")
                time.sleep(wait_interval)
        
        print("[ERROR] ç­‰å¾…è¶…æ—¶")
        return None
    
    def process_audio_file(self, file_url: str, language: str = 'zh-CN') -> List[Dict[str, Any]]:
        """å®Œæ•´å¤„ç†éŸ³é¢‘æ–‡ä»¶ï¼Œè¿”å›å­—å¹•æ ¼å¼æ•°æ®
        
        Args:
            file_url: éŸ³é¢‘æ–‡ä»¶URL
            language: è¯­è¨€ä»£ç 
            
        Returns:
            å­—å¹•å¯¹è±¡æ•°ç»„
        """
        print(f"ğŸ¯ å¼€å§‹ç«å±±å¼•æ“è¯­éŸ³è¯†åˆ«: {file_url}")
        
        # 1. æäº¤ä»»åŠ¡
        job_id = self.submit_audio_file(file_url, language)
        if not job_id:
            return []
        
        # 2. ç­‰å¾…å®Œæˆ
        result = self.wait_for_completion(job_id)
        if not result:
            return []
        
        # 3. è§£æç»“æœ
        subtitles = self.parse_result_to_subtitles(result)
        print(f"[OK] ç«å±±å¼•æ“è¯†åˆ«å®Œæˆï¼Œç”Ÿæˆ {len(subtitles)} æ®µå­—å¹•")
        
        return subtitles
    
    def transcribe_audio_for_silence_detection(self, file_url: str, language: str = 'zh-CN') -> Optional[Dict[str, Any]]:
        """è½¬å½•éŸ³é¢‘ç”¨äºåœé¡¿æ£€æµ‹ï¼ˆè¿”å›åŸå§‹ASRç»“æœï¼‰
        
        Args:
            file_url: éŸ³é¢‘æ–‡ä»¶URL
            language: è¯­è¨€ä»£ç ï¼Œé»˜è®¤ä¸­æ–‡
            
        Returns:
            åŸå§‹ASRç»“æœï¼Œå¤±è´¥è¿”å›None
        """
        print(f"[INFO] è½¬å½•éŸ³é¢‘ç”¨äºåœé¡¿æ£€æµ‹: {file_url}")
        
        try:
            # æäº¤éŸ³é¢‘æ–‡ä»¶
            job_id = self.submit_audio_file(file_url, language)
            if not job_id:
                return None
            
            # ç­‰å¾…è¯†åˆ«å®Œæˆ
            result = self.wait_for_completion(job_id)
            if not result:
                return None
            
            # æ£€æŸ¥è¯†åˆ«ç»“æœ
            if result.get('code') != 0:
                print(f"[ERROR] è¯†åˆ«å¤±è´¥: {result.get('message', 'æœªçŸ¥é”™è¯¯')}")
                return None
            
            utterances = result.get('utterances', [])
            if not utterances:
                print("âš ï¸ æœªè¯†åˆ«åˆ°è¯­éŸ³å†…å®¹")
                return None
            
            print(f"[OK] éŸ³é¢‘è½¬å½•å®Œæˆï¼Œè¯†åˆ«åˆ° {len(utterances)} ä¸ªè¯­éŸ³ç‰‡æ®µ")
            return result
            
        except Exception as e:
            print(f"[ERROR] éŸ³é¢‘è½¬å½•å¤±è´¥: {e}")
            return None
    
    def parse_result_to_subtitles(self, result: Dict[str, Any]) -> List[Dict[str, Any]]:
        """è§£æç«å±±å¼•æ“ç»“æœä¸ºå­—å¹•æ ¼å¼
        
        Args:
            result: ç«å±±å¼•æ“è¯†åˆ«ç»“æœ
            
        Returns:
            å­—å¹•å¯¹è±¡æ•°ç»„
        """
        subtitles = []
        
        try:
            # è·å–è¯†åˆ«æ•°æ® - æ ¹æ®APIæ–‡æ¡£ï¼Œutterancesåœ¨æ ¹å±‚çº§
            utterances = result.get('utterances', [])
            
            if not utterances:
                print("âš ï¸ æœªæ‰¾åˆ°è¯†åˆ«ç»“æœ")
                return []
            
            print(f"[INFO] è§£æ {len(utterances)} ä¸ªè¯­éŸ³ç‰‡æ®µ")
            
            for i, utterance in enumerate(utterances):
                text = utterance.get('text', '').strip()
                start_time = utterance.get('start_time', 0) / 1000.0  # è½¬æ¢ä¸ºç§’
                end_time = utterance.get('end_time', 0) / 1000.0      # è½¬æ¢ä¸ºç§’
                
                if text:
                    # æ¸…ç†æ–‡æœ¬ï¼ˆç§»é™¤æ ‡ç‚¹ç¬¦å·ï¼‰
                    clean_text = self.clean_text(text)
                    
                    subtitle = {
                        'text': clean_text,
                        'start': start_time,
                        'end': end_time
                    }
                    subtitles.append(subtitle)
                    
                    duration = end_time - start_time
                    print(f"   ç¬¬{i+1}æ®µ: [{start_time:.3f}s-{end_time:.3f}s] ({duration:.1f}s) {clean_text}")
            
            return subtitles
            
        except Exception as e:
            print(f"[ERROR] è§£æç»“æœå¼‚å¸¸: {e}")
            return []
    
    def clean_text(self, text: str) -> str:
        """æ¸…ç†æ–‡æœ¬ï¼Œç§»é™¤æ ‡ç‚¹ç¬¦å·"""
        import re
        
        # ç§»é™¤æ ‡ç‚¹ç¬¦å·ï¼Œä¿ç•™ä¸­æ–‡ã€è‹±æ–‡ã€æ•°å­—
        cleaned = re.sub(r'[^\u4e00-\u9fff\w\s]', '', text)
        
        # ç§»é™¤å¤šä½™ç©ºæ ¼
        cleaned = re.sub(r'\s+', ' ', cleaned).strip()
        
        return cleaned
    
    def extract_keywords_with_ai(self, text: str, max_keywords: int = None) -> List[str]:
        """ä½¿ç”¨AIæ™ºèƒ½æå–å…³é”®è¯ï¼Œä¼˜åŒ–ç”¨æˆ·æ³¨æ„åŠ›å’Œç•™å­˜
        
        Args:
            text: è¾“å…¥æ–‡æœ¬
            max_keywords: æœ€å¤§å…³é”®è¯æ•°é‡ï¼ˆå·²ä¼˜åŒ–ä¸ºæ— é™åˆ¶ï¼Œä¿ç•™å‚æ•°å‘åå…¼å®¹ï¼‰
            
        Returns:
            å…³é”®è¯åˆ—è¡¨ï¼ˆä¼˜åŒ–åæ— æ•°é‡é™åˆ¶ï¼ŒåŸºäºå†…å®¹è´¨é‡åŠ¨æ€æå–ï¼‰
        """
        print(f"[INFO] ä½¿ç”¨AIæ™ºèƒ½æå–å…³é”®è¯ï¼ˆæ— é™åˆ¶æ¨¡å¼ï¼‰: {text[:50]}...")
        
        try:
            # æ£€æŸ¥è±†åŒ…APIé…ç½®
            if not self.doubao_token:
                print("âš ï¸ æœªé…ç½®è±†åŒ…API tokenï¼Œä½¿ç”¨æœ¬åœ°æ™ºèƒ½ç®—æ³•")
                return self._fallback_keyword_extraction(text, max_keywords)
            
            # è±†åŒ…APIè¿›è¡Œæ™ºèƒ½å…³é”®è¯æå–ï¼ˆç”¨æˆ·æ³¨æ„åŠ›ä¼˜åŒ–ç‰ˆæœ¬ï¼‰
            response = requests.post(
                'https://ark.cn-beijing.volces.com/api/v3/chat/completions',
                headers={
                    'Content-Type': 'application/json',
                    'Authorization': f'Bearer {self.doubao_token}'  # ä½¿ç”¨è±†åŒ…token
                },
                json={
                    "model": self.doubao_model,  # ä½¿ç”¨è±†åŒ…æ¨¡å‹åç§°
                    "messages": [
                        {
                            "role": "system",
                            "content": "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„è§†é¢‘å†…å®¹ä¼˜åŒ–ä¸“å®¶ï¼Œä¸“æ³¨äºé€šè¿‡å…³é”®è¯é«˜äº®æå‡ç”¨æˆ·æ³¨æ„åŠ›å’Œè§†é¢‘ç•™å­˜ã€‚è¯·ä»ç»™å®šæ–‡æœ¬ä¸­æ™ºèƒ½æå–æ‰€æœ‰æœ‰ä»·å€¼çš„å…³é”®è¯ã€‚\n\næ ¸å¿ƒç­–ç•¥ï¼ˆç”¨æˆ·æ³¨æ„åŠ›æœ€å¤§åŒ–ï¼‰ï¼š\n1. ã€æ•°é‡æ— é™åˆ¶ã€‘æå–æ‰€æœ‰æœ‰æ„ä¹‰çš„è¯æ±‡ï¼Œä¸è®¾æ•°é‡ä¸Šé™\n2. ã€é«˜é¢‘å¸å¼•è¯ä¼˜å…ˆã€‘é‡ç‚¹æ ‡è®°èƒ½å¼•èµ·ç”¨æˆ·æƒ…æ„Ÿå…±é¸£çš„è¯æ±‡\n3. ã€å¤šå±‚æ¬¡è¦†ç›–ã€‘åŒæ—¶æå–ï¼šæ ¸å¿ƒæ¦‚å¿µè¯ã€æƒ…æ„Ÿè§¦å‘è¯ã€è¡ŒåŠ¨å¯¼å‘è¯ã€æ•°å­—é‡‘é¢è¯\n4. ã€ç”¨æˆ·ç—›ç‚¹è¯æ±‡ã€‘ç‰¹åˆ«å…³æ³¨ï¼šé’±ã€èµšé’±ã€æš´å¯Œã€è´¢å¯Œã€æ”¶å…¥ã€æŠ•èµ„ã€æœºä¼šç­‰\n5. ã€æƒ…ç»ªæ¿€å‘è¯æ±‡ã€‘åŒ…å«ï¼šéœ‡æƒŠã€æƒŠäººã€ç§˜å¯†ã€å†…å¹•ã€çœŸç›¸ã€çˆ†æ–™ç­‰\n6. ã€æ—¶é—´ç´§è¿«è¯æ±‡ã€‘å¦‚ï¼šé©¬ä¸Šã€ç«‹å³ã€ç°åœ¨ã€ä»Šå¹´ã€æœªæ¥ã€è¶‹åŠ¿ç­‰\n7. ã€æƒå¨èƒŒä¹¦è¯æ±‡ã€‘å¦‚ï¼šä¸“å®¶ã€å®˜æ–¹ã€æ”¿ç­–ã€å›½å®¶ã€å¤®è¡Œç­‰\n\nè¾“å‡ºè¦æ±‚ï¼š\n- åªè¿”å›å…³é”®è¯ï¼Œç”¨é€—å·åˆ†éš”\n- æŒ‰é‡è¦æ€§æ’åºï¼Œä¼˜å…ˆçº§ï¼šæƒ…æ„Ÿè§¦å‘>æ ¸å¿ƒæ¦‚å¿µ>æ•°é‡è¯æ±‡>ä¸€èˆ¬åè¯\n- é•¿åº¦2-6ä¸ªå­—ï¼Œä¼˜å…ˆ4å­—æˆè¯­å’Œä¸“ä¸šæœ¯è¯­\n- ç¡®ä¿æ¯ä¸ªè¯éƒ½èƒ½æŠ“ä½ç”¨æˆ·çœ¼çƒ\n\nç¤ºä¾‹ï¼š\nè¾“å…¥ï¼š'å¹´è½»äººå¦‚ä½•åœ¨ç»æµä¸æ™¯æ°”æ—¶ä»£å®ç°è´¢å¯Œè‡ªç”±ï¼Œä¸“å®¶å»ºè®®è¿™ä¸‰ä¸ªèµšé’±æœºä¼šä¸å®¹é”™è¿‡'\nè¾“å‡ºï¼šè´¢å¯Œè‡ªç”±,èµšé’±æœºä¼š,ç»æµä¸æ™¯æ°”,å¹´è½»äºº,ä¸“å®¶å»ºè®®,ä¸å®¹é”™è¿‡,å®ç°,è´¢å¯Œ,æœºä¼š,èµšé’±,ç»æµ,æ—¶ä»£,å»ºè®®,å¹´è½»,è‡ªç”±"
                        },
                        {
                            "role": "user",
                            "content": f"è¯·æ™ºèƒ½æå–ä»¥ä¸‹æ–‡æœ¬çš„æ‰€æœ‰æœ‰ä»·å€¼å…³é”®è¯ï¼š{text}"
                        }
                    ],
                    "max_tokens": 500,  # å¢åŠ tokené™åˆ¶ä»¥æ”¯æŒæ›´å¤šå…³é”®è¯
                    "temperature": 0.1  # é™ä½æ¸©åº¦æé«˜ä¸€è‡´æ€§
                }
            )
            
            if response.status_code == 200:
                result = response.json()
                content = result.get('choices', [{}])[0].get('message', {}).get('content', '')
                
                # è§£æå…³é”®è¯ï¼ˆæ— æ•°é‡é™åˆ¶ï¼‰
                keywords = [kw.strip() for kw in content.split(',') if kw.strip() and len(kw.strip()) >= 2]
                
                # å»é‡ä½†ä¿æŒé¡ºåº
                seen = set()
                unique_keywords = []
                for kw in keywords:
                    if kw not in seen and kw:
                        seen.add(kw)
                        unique_keywords.append(kw)
                
                print(f"[OK] AIæ™ºèƒ½æå–å…³é”®è¯ï¼ˆ{len(unique_keywords)}ä¸ªï¼‰: {unique_keywords}")
                return unique_keywords
            else:
                print(f"[ERROR] AIå…³é”®è¯æå–å¤±è´¥: {response.status_code}, {response.text}")
                print("[INFO] ä½¿ç”¨æœ¬åœ°æ™ºèƒ½ç®—æ³•ä½œä¸ºå¤‡ç”¨")
                return self._fallback_keyword_extraction(text, max_keywords)
                
        except Exception as e:
            print(f"[ERROR] AIå…³é”®è¯æå–å¼‚å¸¸: {e}")
            print("[INFO] ä½¿ç”¨æœ¬åœ°æ™ºèƒ½ç®—æ³•ä½œä¸ºå¤‡ç”¨")
            return self._fallback_keyword_extraction(text, max_keywords)
    
    def _fallback_keyword_extraction(self, text: str, max_keywords: int = None) -> List[str]:
        """å¤‡ç”¨å…³é”®è¯æå–æ–¹æ³•ï¼ˆæ™ºèƒ½æ— é™åˆ¶ç‰ˆæœ¬ï¼Œç”¨æˆ·æ³¨æ„åŠ›ä¼˜åŒ–ï¼‰
        
        Args:
            text: è¾“å…¥æ–‡æœ¬
            max_keywords: ä¿ç•™å‚æ•°ä»¥å‘åå…¼å®¹ï¼ˆå®é™…ä¸é™åˆ¶æ•°é‡ï¼‰
            
        Returns:
            å…³é”®è¯åˆ—è¡¨ï¼ˆåŸºäºå†…å®¹ä»·å€¼åŠ¨æ€æå–ï¼Œæ— æ•°é‡é™åˆ¶ï¼‰
        """
        print("[INFO] ä½¿ç”¨å¤‡ç”¨å…³é”®è¯æå–æ–¹æ³•ï¼ˆæ™ºèƒ½æ— é™åˆ¶ç‰ˆæœ¬ï¼‰")
        
        import re
        from collections import Counter
        
        # ç§»é™¤æ ‡ç‚¹ç¬¦å·ï¼Œåªä¿ç•™ä¸­æ–‡å’Œè‹±æ–‡
        cleaned_text = re.sub(r'[^\u4e00-\u9fff\w\s]', '', text)
        
        # å¤§å¹…æ‰©å±•çš„é«˜ä»·å€¼è¯æ±‡æ¨¡å¼ï¼ˆç”¨æˆ·æ³¨æ„åŠ›ä¼˜åŒ–ï¼‰
        high_value_patterns = [
            # è´¢å¯Œç›¸å…³ï¼ˆç”¨æˆ·ç—›ç‚¹ï¼‰- æœ€é«˜ä¼˜å…ˆçº§
            r'(åƒä¸‡å¯Œç¿|è´¢å¯Œè‡ªç”±|æš´å¯Œ|èµšé’±|æŠ•èµ„ç†è´¢|ç»æµæ¡ä»¶|æ”¶å…¥|é‡‘é’±|èµ„äº§|ç†è´¢|æŠ•èµ„|è‚¡ç¥¨|æˆ¿äº§|åˆ›ä¸š|å•†æœº)',
            
            # æƒ…æ„Ÿè§¦å‘è¯ï¼ˆå¸å¼•æ³¨æ„åŠ›ï¼‰
            r'(éœ‡æƒŠ|æƒŠäºº|ç§˜å¯†|å†…å¹•|çœŸç›¸|çˆ†æ–™|æ­ç§˜|ç‹¬å®¶|é¦–æ¬¡|å²ä¸Š|ç©ºå‰|ç»æ— ä»…æœ‰|å‰æ‰€æœªæœ‰|é‡ç£…|çªç ´æ€§)',
            
            # æ—¶é—´ç´§è¿«æ€§ï¼ˆåˆ¶é€ ç„¦è™‘æ„Ÿï¼‰
            r'(é©¬ä¸Š|ç«‹å³|ç°åœ¨|ä»Šå¹´|æ˜å¹´|æœªæ¥|å³å°†|æ­£åœ¨|å¿«é€Ÿ|è¿…é€Ÿ|æ€¥éœ€|ç´§æ€¥|é™æ—¶|å€’è®¡æ—¶|æœ€åæœºä¼š)',
            
            # æƒå¨èƒŒä¹¦ï¼ˆå¢åŠ å¯ä¿¡åº¦ï¼‰
            r'(ä¸“å®¶|æƒå¨|å®˜æ–¹|æ”¿åºœ|å›½å®¶|å¤®è¡Œ|æ”¿ç­–|æ³•å¾‹|è§„å®š|ç ”ç©¶|æŠ¥å‘Š|æ•°æ®|è°ƒæŸ¥|ç»Ÿè®¡|ç§‘å­¦|å­¦è€…)',
            
            # è¡Œä¸šçƒ­ç‚¹ï¼ˆæŠ“ä½è¶‹åŠ¿ï¼‰
            r'(äººå·¥æ™ºèƒ½|ç§‘æŠ€å‘å±•|äº’è”ç½‘|å¤§æ•°æ®|åŒºå—é“¾|æ–°èƒ½æº|æˆ¿åœ°äº§|æ•™è‚²|åŒ»ç–—|å…»è€|æ¶ˆè´¹å‡çº§)',
            
            # ç¤¾ä¼šçƒ­ç‚¹ï¼ˆå¼•èµ·å…±é¸£ï¼‰
            r'(å¹´è½»äºº|ä¸­å¹´äºº|è€å¹´äºº|ä¸Šç­æ—|å­¦ç”Ÿ|å®¶é•¿|åŸå¸‚|å†œæ‘|ç¤¾ä¼š|æ°‘ç”Ÿ|å°±ä¸š|æ•™è‚²|åŒ»ç–—|å…»è€)',
            
            # æ•°å­—é‡‘é¢ï¼ˆå…·ä½“åŒ–ï¼‰
            r'(ä¸€åƒä¸‡|åƒä¸‡|ç™¾ä¸‡|åä¸‡|ä¸‡å…ƒ|å…ƒ|äº¿|åƒäº¿|ä¸‡äº¿|å€|ç™¾åˆ†|æŠ˜æ‰£|ä¼˜æƒ |å…è´¹|è¡¥è´´|å¥–åŠ±)',
            
            # åŠ¨ä½œå¯¼å‘è¯ï¼ˆå¼•å¯¼è¡Œä¸ºï¼‰
            r'(å­¦ä¼š|æŒæ¡|è·å¾—|å®ç°|è¾¾åˆ°|æå‡|æ”¹å–„|ä¼˜åŒ–|è§£å†³|å…‹æœ|é¿å…|é˜²èŒƒ|æŠ“ä½|æŠŠæ¡|é€‰æ‹©)',
            
            # å¯¹æ¯”å¼ºè°ƒï¼ˆçªå‡ºé‡è¦æ€§ï¼‰
            r'(æœ€å¥½|æœ€ä½³|æœ€ä¼˜|é¡¶çº§|ä¸€æµ|é«˜ç«¯|ä½ç«¯|æ™®é€š|å¹³å‡|æ ‡å‡†|ç‰¹æ®Š|ç‹¬ç‰¹|å”¯ä¸€|ç¨€æœ‰|çè´µ)',
            
            # é—®é¢˜ç—›ç‚¹ï¼ˆå¼•èµ·å…±é¸£ï¼‰
            r'(é—®é¢˜|å›°éš¾|æŒ‘æˆ˜|å±æœº|é£é™©|é™·é˜±|è¯¯åŒº|ç›²åŒº|æ¼æ´|ç¼ºé™·|ä¸è¶³|ç¼ºç‚¹|å¼Šç«¯|éšæ‚£)',
            
            # è§£å†³æ–¹æ¡ˆï¼ˆæä¾›ä»·å€¼ï¼‰
            r'(æ–¹æ³•|æŠ€å·§|ç§˜è¯€|ç­–ç•¥|æ–¹æ¡ˆ|å»ºè®®|æŒ‡å¯¼|æ•™ç¨‹|æ”»ç•¥|å®å…¸|æ‰‹å†Œ|æŒ‡å—|ç»éªŒ|å¿ƒå¾—)',
            
            # 4å­—æˆè¯­å’Œå›ºå®šæ­é…
            r'(æ‹†è¿æ”¹é€ |è´§å¸åŒ–å®‰ç½®|åŸä¸­æ‘|è€æ—§å°åŒº|å¤šæ‹†å°‘å»º|æ‹†å°å»ºå¤§|æ”¹å–„å‹|èˆ’é€‚å‹|ä½æˆ¿ç»“æ„|å†…éœ€|ç»æµ|æ—¶ä»£çº¢åˆ©)',
            
            # 3å­—é‡è¦è¯æ±‡
            r'(æ‹†è¿æˆ·|è¡¥å¿æ¬¾|åº“å­˜|ä¾›åº”|å“è´¨|å‡çº§|åˆšéœ€|è§„åˆ’|è‡´å¯Œ|å®ˆä½|è´¢å¯Œ|æš´å¯Œ|è½¬çœ¼|å½’é›¶|æ”¿ç­–|æ–¹å‘)',
            
            # é€šç”¨2å­—è¯æ±‡
            r'(æ‹†è¿|æ”¹é€ |å®‰ç½®|è¡¥å¿|ç°é‡‘|å‘æ”¾|è·å¾—|æˆä¸º|å…³é”®|é‡è¦|å˜åŒ–|ä¸åŒ|ä¸¥æ§|æ¨åŠ¨|æ¿€æ´»|å¸¦åŠ¨|ç†æ€§|æŠŠæ¡)'
        ]
        
        candidate_words = []
        
        # ä½¿ç”¨é«˜ä»·å€¼æ¨¡å¼æå–ï¼ˆæŒ‰ä¼˜å…ˆçº§ï¼‰
        for pattern in high_value_patterns:
            matches = re.findall(pattern, cleaned_text)
            candidate_words.extend(matches)
        
        # è¡¥å……é€šç”¨ä¸­æ–‡è¯æ±‡ï¼ˆ2-6å­—ï¼‰
        general_words = re.findall(r'[\u4e00-\u9fff]{2,6}', cleaned_text)
        candidate_words.extend(general_words)
        
        # å¤§å¹…ç¼©å‡åœç”¨è¯åˆ—è¡¨ï¼Œæ›´å¤šè¯æ±‡å‚ä¸é«˜äº®
        stop_words = {
            # åªè¿‡æ»¤æœ€åŸºç¡€çš„æ— æ„ä¹‰è¯æ±‡
            'è¿™ä¸ª', 'é‚£ä¸ª', 'è¿™æ ·', 'é‚£æ ·', 'å› ä¸º', 'æ‰€ä»¥', 'ä½†æ˜¯', 'ç„¶å', 'å¦‚æœ', 
            'çš„è¯', 'å°±æ˜¯', 'è¿˜æ˜¯', 'ä¸è¿‡', 'è™½ç„¶', 'ä¸€äº›', 'ä¸€ä¸ª', 'ä»€ä¹ˆ', 'æ€ä¹ˆ', 
            'ä¸ºä»€ä¹ˆ', 'å“ªé‡Œ', 'æ—¶å€™', 'ç°åœ¨', 'ä»¥å‰', 'ä»¥å'
        }
        
        # è¿‡æ»¤å¤„ç†
        filtered_words = []
        for word in candidate_words:
            if (word not in stop_words and 
                len(word) >= 2 and 
                not re.match(r'^[\u4e00-\u9fff]{1}[çš„äº†ç€è¿‡åœ¨]$', word)):  # è¿‡æ»¤åŠ©è¯ç»“å°¾
                filtered_words.append(word)
        
        # ç»Ÿè®¡è¯é¢‘ï¼Œä¼˜å…ˆé•¿è¯å’Œé«˜é¢‘è¯
        word_freq = Counter(filtered_words)
        
        # æŒ‰é‡è¦æ€§æ’åºï¼šè¯é¢‘ Ã— é•¿åº¦æƒé‡
        sorted_words = sorted(word_freq.items(), 
                            key=lambda x: (x[1] * (1 + len(x[0]) * 0.2)), 
                            reverse=True)
        
        # é¢„å®šä¹‰çš„è¶…é«˜ä»·å€¼å…³é”®è¯ï¼ˆå¿…é¡»é«˜äº®ï¼‰
        must_highlight_keywords = [
            # è´¢å¯Œç›¸å…³
            'åƒä¸‡å¯Œç¿', 'è´¢å¯Œè‡ªç”±', 'æ‹†è¿æš´å¯Œ', 'è¡¥å¿æ¬¾', 'æ•°ç™¾ä¸‡å…ƒ', 'ä¸Šåƒä¸‡å…ƒ',
            # æ”¿ç­–å˜åŒ–
            'è´§å¸åŒ–å®‰ç½®', 'åŸä¸­æ‘', 'è€æ—§å°åŒº', 'å¤šæ‹†å°‘å»º', 'æ‹†å°å»ºå¤§',
            # æŠ•èµ„ç†è´¢
            'ç¨³å¥é…ç½®', 'æ”¹å–„ä½æˆ¿', 'ç›²ç›®æ¶ˆè´¹', 'æŠ•æœº',
            # æ—¶é—´æ•æ„Ÿ
            'äºŒé›¶äºŒäº”å¹´', 'ä¸‰åä¸ªåŸå¸‚', 'ä¸‰ç™¾ä¸ª', 'ä¸€å¤œæš´å¯Œ', 'è½¬çœ¼å½’é›¶',
            # é‡è¦æ¦‚å¿µ
            'æ‹†è¿æ”¹é€ ', 'å…¨é¢æ¨è¿›', 'é‡æ–°æå€¡', 'æ—¶ä»£çº¢åˆ©', 'æ”¿ç­–æ–¹å‘'
        ]
        
        # æ„å»ºæœ€ç»ˆå…³é”®è¯åˆ—è¡¨
        final_keywords = []
        
        # ç¬¬ä¸€ä¼˜å…ˆçº§ï¼šå¿…é¡»é«˜äº®çš„è¶…é«˜ä»·å€¼è¯
        for must_word in must_highlight_keywords:
            if must_word in text and must_word not in final_keywords:
                final_keywords.append(must_word)
        
        # ç¬¬äºŒä¼˜å…ˆçº§ï¼šä»é¢‘ç‡ç»Ÿè®¡ä¸­é€‰æ‹©
        for word, freq in sorted_words:
            if word not in final_keywords:
                # æ£€æŸ¥é‡å¤ä½†å…è®¸æ›´å¤šå˜ä½“
                is_duplicate = False
                for existing in final_keywords:
                    if (len(word) <= 2 and len(existing) <= 2 and 
                        (word == existing or word in existing or existing in word)):
                        is_duplicate = True
                        break
                
                if not is_duplicate:
                    final_keywords.append(word)
        
        # æŒ‰æ–‡æœ¬å‡ºç°é¡ºåºé‡æ–°æ’åˆ—ï¼ˆä¿æŒé˜…è¯»è‡ªç„¶æ€§ï¼‰
        text_order_keywords = []
        for word in final_keywords:
            if word in text:
                pos = text.find(word)
                text_order_keywords.append((pos, word))
        
        # æ’åºå¹¶æå–è¯æ±‡
        text_order_keywords.sort(key=lambda x: x[0])
        final_ordered_keywords = [word for pos, word in text_order_keywords]
        
        print(f"[INFO] æ™ºèƒ½æ— é™åˆ¶æå–å…³é”®è¯ï¼ˆ{len(final_ordered_keywords)}ä¸ªï¼‰: {final_ordered_keywords}")
        return final_ordered_keywords


def test_volcengine_asr():
    """æµ‹è¯•ç«å±±å¼•æ“ASRåŠŸèƒ½"""
    
    print("ğŸ§ª ç«å±±å¼•æ“ASRæµ‹è¯•")
    print("=" * 50)
    
    # ä½¿ç”¨æ‚¨æä¾›çš„å‡­æ®
    appid = "6046310832"
    access_token = "fMotJVOsyk6K_dDRoqM14kGdMJYBrcJY"
    
    # åˆ›å»ºASRå®¢æˆ·ç«¯
    asr = VolcengineASR(appid, access_token)
    
    # æµ‹è¯•éŸ³é¢‘æ–‡ä»¶URLï¼ˆæ‚¨éœ€è¦æä¾›ä¸€ä¸ªå¯è®¿é—®çš„éŸ³é¢‘URLï¼‰
    file_url = "https://oss.oemi.jdword.com/prod/temp/srt/V20250901152556001.wav"
    
    print(f"[INFO] æµ‹è¯•éŸ³é¢‘: {file_url}")
    print("[INFO] é¢„æœŸç»“æœ: ç”Ÿæˆç²¾ç¡®çš„å­—å¹•æ—¶é—´æˆ³")
    
    try:
        # å¤„ç†éŸ³é¢‘æ–‡ä»¶
        subtitles = asr.process_audio_file(file_url)
        
        if subtitles:
            print(f"\n[OK] è¯†åˆ«æˆåŠŸ! ç”Ÿæˆ {len(subtitles)} æ®µå­—å¹•")
            print("\nğŸ“‹ å­—å¹•å†…å®¹:")
            print("-" * 40)
            
            for i, subtitle in enumerate(subtitles, 1):
                duration = subtitle['end'] - subtitle['start']
                print(f"{i:2d}. [{subtitle['start']:6.3f}s-{subtitle['end']:6.3f}s] ({duration:4.1f}s) {subtitle['text']}")
            
            # ç”ŸæˆSRTæ–‡ä»¶
            srt_content = generate_srt(subtitles)
            
            output_path = "volcengine_test_result.srt"
            with open(output_path, 'w', encoding='utf-8') as f:
                f.write(srt_content)
            
            print(f"\nğŸ“ SRTæ–‡ä»¶å·²ä¿å­˜: {output_path}")
            
        else:
            print("[ERROR] è¯†åˆ«å¤±è´¥")
            
    except Exception as e:
        print(f"[ERROR] æµ‹è¯•å¼‚å¸¸: {e}")
        import traceback
        traceback.print_exc()


def generate_srt(subtitles: List[Dict[str, Any]]) -> str:
    """ç”ŸæˆSRTæ ¼å¼å†…å®¹"""
    srt_content = []
    
    for i, subtitle in enumerate(subtitles, 1):
        start_time = subtitle['start']
        end_time = subtitle['end']
        text = subtitle['text']
        
        # è½¬æ¢ä¸ºSRTæ—¶é—´æ ¼å¼
        start_srt = seconds_to_srt_time(start_time)
        end_srt = seconds_to_srt_time(end_time)
        
        srt_content.append(f"{i}")
        srt_content.append(f"{start_srt} --> {end_srt}")
        srt_content.append(text)
        srt_content.append("")
    
    return "\n".join(srt_content)


def seconds_to_srt_time(seconds: float) -> str:
    """å°†ç§’æ•°è½¬æ¢ä¸ºSRTæ—¶é—´æ ¼å¼"""
    hours = int(seconds // 3600)
    minutes = int((seconds % 3600) // 60)
    secs = int(seconds % 60)
    milliseconds = int((seconds % 1) * 1000)
    
    return f"{hours:02d}:{minutes:02d}:{secs:02d},{milliseconds:03d}"


if __name__ == "__main__":
    test_volcengine_asr()
